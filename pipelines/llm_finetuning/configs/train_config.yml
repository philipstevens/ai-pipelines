# Example training configuration for LLM fine-tuning
base_model: "meta-llama/Llama-3-8B-Instruct"
train_path: "data/processed/train.jsonl"
val_path: "data/processed/val.jsonl"
output_dir: "models/finetuned-llama3/"
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
num_train_epochs: 3
per_device_train_batch_size: 4
learning_rate: 2e-4
load_in_4bit: true
bf16: true
max_seq_length: 2048
