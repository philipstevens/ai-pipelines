{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  AI Pipelines: LLM Fine-Tuning Demo\n",
    "This Colab notebook demonstrates how to fine-tune an open LLM using the **AI Pipelines** framework.\n",
    "\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Clone the repo\n",
    "3. Prepare a sample dataset\n",
    "4. Run fine-tuning\n",
    "5. Evaluate and generate text"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/philipstevens/ai-pipelines.git\n",
    "%cd ai-pipelines\n",
    "!pip install -r <(conda env export --no-builds | grep -v '^prefix' | grep '^- ' | sed 's/^- //' | grep -v '^python')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# (Optional) Create toy dataset\n",
    "import json, os\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "samples = [\n",
    "    {\"instruction\": \"Explain the difference between AI and ML.\", \"response\": \"AI is the broad field of building intelligent systems; ML is a subset focused on data-driven learning.\"},\n",
    "    {\"instruction\": \"What is LoRA fine-tuning?\", \"response\": \"LoRA adapts model weights efficiently using low-rank matrices, reducing memory footprint.\"}\n",
    "]\n",
    "with open('data/raw/sample.jsonl', 'w') as f:\n",
    "    for s in samples:\n",
    "        json.dump(s, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Process data\n",
    "!python pipelines/llm_finetuning/scripts/run_data_pipeline.py --input_dir data/raw"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run fine-tuning (this will use the example config)\n",
    "!python pipelines/llm_finetuning/scripts/run_finetune.py --config pipelines/llm_finetuning/configs/example_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate model latency improvements\n",
    "!python pipelines/llm_finetuning/scripts/run_eval.py --model_before meta-llama/Llama-3-8B-Instruct --model_after models/finetuned-llama3/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try inference\n",
    "from pipelines.llm_finetuning.serve.model_loader import load_model, generate\n",
    "model, tokenizer = load_model('models/finetuned-llama3/')\n",
    "print(generate(model, tokenizer, 'Explain how transformers work.', max_tokens=128))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AI Pipelines LLM Fine-Tuning Demo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
